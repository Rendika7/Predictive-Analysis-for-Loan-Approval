# -*- coding: utf-8 -*-
"""Predictive Analysis for Loan Approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14FYwewt8WB7ulcCopa1JaiFhFV7JgoRF

<h1> Submission Dicoding Predictive Analysis for Loan Approval </h1>

*   ```Nama Lengkap:``` **Rendika Nurhartanto Suharto**
*   ```Username:``` **rendika7**
*   ```Email:``` **rendikarendi96@gmail.com**
---

# **1. Library and Function Needed**

Tahap `Library and Function Needed` meliputi:

1. **Import Library**: Mengimpor berbagai pustaka untuk manipulasi data (`pandas`, `numpy`), visualisasi (`matplotlib`, `seaborn`, `plotly`), preprocessing, evaluasi, model machine learning (`sklearn`), serta otomatisasi pemodelan (`pycaret`).

2. **Mount Google Drive**: Untuk akses file di Google Colab.

3. **Fungsi Analisis dan Manipulasi Data**:
   - `check_duplicates()`: Memeriksa data duplikat dalam DataFrame.
   - `missing_data()`: Memeriksa nilai yang hilang.
   - `basic_data_info()`: Menampilkan informasi dasar, statistik deskriptif, dan visualisasi tipe data dari DataFrame.

4. **Suppress Warnings**: Mengabaikan peringatan selama eksekusi.

5. **Konfigurasi Pandas**: Mengatur agar semua kolom ditampilkan saat melihat DataFrame.

Semua ini mempersiapkan library/alat untuk eksplorasi, analisis, dan pemodelan data.
"""

!pip install dask[dataframe]
!pip install pycaret

import os                     # Untuk operasi terkait file dan direktori
import pandas as pd            # Untuk manipulasi data dan analisis (DataFrame)
import numpy as np             # Untuk komputasi numerik (array, operasi matematis)
from collections import Counter                 # Untuk menghitung jumlah objek
import time                   # Untuk mengukur waktu
import math                   # Untuk operasi matematika

# Untuk visualisasi data (plot)
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Preprocessing Tools
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder # Tranformation Data
from sklearn.model_selection import train_test_split # Splitting Data

# Models and Evaluation
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, classification_report # Evaluation Metrics for Classification Cases
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import RidgeClassifier, LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV # Hyperparameter Tuning

# Suppress Warnings
import warnings
warnings.filterwarnings("ignore") # Untuk mengabaikan peringatan yang mungkin muncul selama proses eksekusi

# PyCaret Classification Module
from pycaret.classification import *

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
# Untuk menghubungkan Google Drive ke Google Colab agar bisa mengakses file langsung dari Drive

pd.set_option("display.max_columns", None) # Mengatur agar semua kolom ditampilkan
# pd.set_option("display.max_row", None)

def check_duplicates(dataframe, kolom=None):
    """
    Memeriksa nilai duplikat dari DataFrame.
    Menampilkan jumlah duplikat dan 10 nilai duplikat teratas (jika ada).
    Args:
        dataframe (pd.DataFrame): DataFrame yang akan diperiksa.
        kolom (list, optional): Kolom-kolom spesifik yang ingin diperiksa. Jika None, diperiksa semua kolom.
    """
    # Menampilkan nilai duplikat
    print("Nilai Duplikat (10 Teratas):")
    duplicate_values = dataframe[dataframe.duplicated(subset=kolom, keep=False)]
    duplicate_count = duplicate_values.shape[0]
    print(f"Jumlah Duplikat data: {duplicate_count}")
    if duplicate_count > 0:
        display(duplicate_values.head(10))
    else:
        print("Tidak ada duplikat yang ditemukan.")
    print("-" * 30)

def missing_data(data):
    """
    Memeriksa nilai yang hilang di DataFrame.
    Args:
        data (pd.DataFrame): DataFrame yang akan diperiksa.
    Returns:
        pd.DataFrame: DataFrame berisi total dan persentase nilai yang hilang.
    """
    total = data.isnull().sum().sort_values(ascending=False)
    percent = (data.isnull().sum() / data.isnull().count() * 100).sort_values(ascending=False).round(3)
    missing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

    print(f"\nTotal Kolom dengan Missing Values: {(total > 0).sum()}")
    print("-" * 30)

    return missing_df

def basic_data_info(dataframe):
    """
    Menampilkan informasi dasar dari DataFrame.
    Args:
        dataframe (pd.DataFrame): DataFrame yang akan ditampilkan informasinya.
    """
    # Menampilkan preview 5 baris pertama dari DataFrame
    print("Preview Data (5 Teratas):")
    print("-" * 30)
    display(dataframe.head())

    # Menampilkan info umum DataFrame
    print("\nInformasi Umum:")
    print("-" * 30)
    print(dataframe.info())

    # Menampilkan statistik deskriptif
    print("\nStatistik Deskriptif:")
    print("-" * 30)
    display(dataframe.describe().T)  # Transpose untuk lebih rapi

    # Menampilkan info tipe data unik di setiap kolom
    print("\nJumlah Nilai Unik per Kolom:")
    print("-" * 30)
    unique_counts = dataframe.nunique().sort_values(ascending=False)
    display(unique_counts)

    # Visualisasi distribusi tipe data
    plt.figure(figsize=(8, 5))
    sns.countplot(x=dataframe.dtypes, palette='Set2')
    plt.title("Distribusi Tipe Data")
    plt.xlabel("Tipe Data")
    plt.ylabel("Jumlah Kolom")
    plt.show()

"""# **2. Data Loading**

Tahap `Data Loading` memuat dataset bernama `Loan.csv` dari Google Drive menggunakan pandas.

- `mainPath` adalah jalur utama menuju direktori proyek di Google Drive.
- `dataPath` adalah jalur menuju folder `Dataset` dalam direktori proyek.
- `df` membaca file CSV `Loan.csv` dari `dataPath` dan menyimpannya sebagai DataFrame.
"""

mainPath = "/content/drive/MyDrive/Colab Notebooks/9.2 DBS Foundation x Dicoding Kelas Expert 2024/Submission - Proyek Pertama/"
dataPath = mainPath + "Dataset/"

df = pd.read_csv(dataPath + "Loan.csv")

"""# **3. General Information in Dataset**

- Kode `basic_data_info(df)` menampilkan informasi umum dari dataset `df`:

1. **Preview Data**: Menampilkan 5 baris pertama dataset.
2. **Informasi Umum**: Tipe data tiap kolom, jumlah nilai yang tidak null, dan ukuran data.
3. **Statistik Deskriptif**: Menampilkan statistik dasar (mean, min, max, dsb.) untuk kolom numerik.
4. **Jumlah Nilai Unik**: Menampilkan jumlah nilai unik per kolom.
5. **Visualisasi Tipe Data**: Plot jumlah kolom berdasarkan tipe datanya.

- Distribusi Variabel Numerik:

    Ditampilkan menggunakan `histogram` dan `boxplot` untuk setiap kolom numerik. Memberikan gambaran tentang `penyebaran dan outlier` di setiap variabel.

- Distribusi Variabel Target ('LoanApproved'):

    `Bar chart` menunjukkan jumlah `persetujuan pinjaman` (disetujui vs tidak disetujui). Memudahkan untuk melihat `keseimbangan kelas target`.
"""

basic_data_info(df)

# Plot distribution of numerical variables
numerical_columns_temp = df.select_dtypes(include=['number']).columns

plt.figure(figsize=(30, 50))
for i, col in enumerate(numerical_columns_temp, start=1):
    plt.subplot(len(numerical_columns_temp), 2, 2 * i - 1)
    sns.histplot(df[col], kde=True)
    plt.title(f"Histogram of {col}", fontsize=16)
    plt.xlabel(col, fontsize=14)
    plt.ylabel('Frequency', fontsize=14)

    plt.subplot(len(numerical_columns_temp), 2, 2 * i)
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot of {col}", fontsize=16)
    plt.xlabel(col, fontsize=14)

plt.tight_layout()
plt.show()

# Plot bar chart for the target variable 'LoanApproved'
plt.figure(figsize=(10, 6))
loan_approved_counts = df['LoanApproved'].value_counts()
sns.barplot(x=loan_approved_counts.index, y=loan_approved_counts.values, palette='viridis')
plt.xlabel("Loan Approved")
plt.ylabel("Count")
plt.title("Loan Approved Distribution")
plt.xticks(ticks=[0, 1], labels=['Not Approved', 'Approved'])
for i, v in enumerate(loan_approved_counts.values):
    plt.text(i, v + 0.5, str(v), ha='center', fontsize=12)
plt.show()

"""# **4. Data Processing**

Tahap pada `data processing` mencakup proses pengolahan data untuk dataset `df`. Berikut adalah ringkasan dari setiap bagian:

1. **Shape Dataset**
   - `df.shape`: Menampilkan dimensi (jumlah baris dan kolom) dari DataFrame.

2. **Data Duplicated**
   - `check_duplicates(df)`: Memeriksa dan menampilkan data duplikat dalam dataset.

3. **Missing Values**
   - `missing_data(df)`: Memeriksa dan menampilkan nilai yang hilang dalam dataset.

4. **Handling Outliers**
   - Memisahkan kolom menjadi numerik dan kategorikal, dan menghitung nilai unik serta persentase keunikannya untuk setiap kolom numerik.
   - Menampilkan boxplot untuk setiap kolom numerik untuk visualisasi outlier.
   - **Fungsi `detect_and_drop_outliers_iqr(numerical_columns, df)`**:
     - Menggunakan metode Interquartile Range (IQR) untuk mendeteksi dan menghapus outlier dari kolom numerik.
     - Mengembalikan DataFrame yang telah dibersihkan dan informasi mengenai outlier yang terdeteksi.

5. **Hasil Akhir**
   - Menampilkan jumlah dan persentase outlier di setiap kolom numerik.
   - Menampilkan jumlah total outlier yang terdeteksi dan sisa data setelah penghapusan outlier.
   - Menghapus kolom `PreviousLoanDefaults` dan `BankruptcyHistory` dari DataFrame `cleaned_df` karena hanya memiliki satu nilai unik setelah penghapusan outlier.

Proses ini bertujuan untuk membersihkan dan menyiapkan data agar siap untuk analisis lebih lanjut atau pemodelan.
"""

df.shape

"""## ***4.1 Duplicated Data***"""

check_duplicates(df)

"""## ***4.2 Missing Value (NaN)***"""

missing_data(df)

"""## ***4.3 Handling Outlier***"""

# Memisahkan kolom numerik dan kategorikal
numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()
numerical_columns.remove("LoanApproved") # Hapus LoanApproved karena ini adalah feature target-nya (label)
categorical_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()

print("Kolom Numerik:", numerical_columns)
print("Kolom Kategorikal:", categorical_columns)

# Menghitung nilai unik (uniqueness) untuk setiap kolom numerik
unique_values = df[numerical_columns].nunique()

# Menghitung persentase uniqueness untuk setiap kolom
total_rows = len(df)
unique_percentage = (unique_values / total_rows) * 100

# Menyusun hasil dalam satu DataFrame
uniqueness_df = pd.DataFrame({
    'Unique Values': unique_values,
    'Percentage (%)': unique_percentage.round(2)  # Membulatkan hingga 2 desimal
})

# Menampilkan hasil
print("Nilai dan Persentase Unik dari Setiap Kolom Numerik:")
display(uniqueness_df)

# Mengatur ukuran grid plot
n_cols = 2
n_rows = len(numerical_columns) // n_cols + 1

plt.figure(figsize=(15, 5 * n_rows))

# Membuat boxplot untuk setiap kolom numerik
for i, col_name in enumerate(numerical_columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(y=df[col_name])
    plt.title(col_name)

plt.tight_layout() # Menyesuaikan tata letak untuk spasi yang lebih baik
plt.show() # Menampilkan plot


# # Buat Boxplot untuk Kolom Numerik
# plt.figure(figsize=(15, 8))
# df[numerical_columns].boxplot()
# plt.xticks(rotation=90)  # Memutar label sumbu x agar lebih mudah dibaca
# plt.title("Boxplot untuk Kolom Numerik")
# plt.show()

def detect_and_drop_outliers_iqr(numerical_columns, df):
    outlier_info = {}  # Untuk menyimpan informasi outlier dari setiap kolom

    # Loop untuk setiap kolom numerik
    for column in numerical_columns:
        # Menghitung Q1 (kuartil pertama) dan Q3 (kuartil ketiga)
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1  # Menghitung Interquartile Range (IQR)

        # Menentukan batas bawah dan batas atas
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Mengidentifikasi outliers (nilai yang berada di luar batas bawah dan atas)
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

        # Menyimpan jumlah outliers untuk kolom tersebut
        outlier_count = len(outliers)
        total_count = len(df[column])  # Total jumlah data dalam kolom
        outlier_info[column] = {
            'Outlier Count': outlier_count,
            'Percentage (%)': (outlier_count / total_count) * 100  # Menghitung persentase outliers
        }

    # Mengonversi informasi outliers menjadi DataFrame untuk ditampilkan
    outlier_df = pd.DataFrame.from_dict(outlier_info, orient='index').reset_index()
    outlier_df.columns = ['Column', 'Outlier Count', 'Percentage (%)']
    outlier_df = outlier_df.sort_values(by='Outlier Count', ascending=False)

    # Menghapus outliers dari DataFrame
    cleaned_df = df.copy()  # Salin DataFrame untuk menghindari modifikasi asli
    for column in numerical_columns:
        # Menghitung ulang Q1, Q3, dan IQR untuk kolom tersebut
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1

        # Menghapus data yang berada di luar batas bawah dan atas
        cleaned_df = cleaned_df[(cleaned_df[column] >= (Q1 - 1.5 * IQR)) &
                                (cleaned_df[column] <= (Q3 + 1.5 * IQR))]

    return cleaned_df, outlier_df  # Mengembalikan DataFrame tanpa outliers dan informasi outlier

cleaned_df, outlier_info_df = detect_and_drop_outliers_iqr(numerical_columns, df)

# Menampilkan informasi outlier untuk setiap kolom numerik
print("Jumlah dan Persentase Outliers di Setiap Kolom Numerik:")
display(outlier_info_df)

print("Jumlah Outlier yang terdeteksi:", outlier_info_df["Outlier Count"].sum())
print("Jumlah Sisa Data setelah terhapus karena handling outlier:", cleaned_df.shape)

cleaned_df.drop(columns=["PreviousLoanDefaults", "BankruptcyHistory"], inplace = True) # Setelah drop outlier, ternyata hanya ada 1 value saja. jadi lebih baik saya drop

"""# **5. Exploratory Data Analysis - EDA**

Tahap ini berfokus pada **Analisis Data Eksploratori (Exploratory Data Analysis - EDA)** untuk menajawab rumusan masalah yaitu memahami faktor-faktor yang memengaruhi persetujuan pinjaman (`LoanApproved`). Berikut adalah ringkasan dari sub-bagian yang dilakukan:

1. **Korelasi antara Fitur dan Persetujuan Pinjaman**
   - Menghapus kolom yang tidak relevan seperti `ApplicationDate` dari dataset bersih (`cleaned_df`).
   - Mengonversi kolom kategorikal menjadi numerik menggunakan `LabelEncoder`.
   - Menghitung matriks korelasi untuk kolom numerik dalam DataFrame.
   - Mengurutkan korelasi terhadap kolom `LoanApproved` dan memvisualisasikannya menggunakan diagram batang horizontal untuk menunjukkan fitur-fitur yang memiliki pengaruh terhadap persetujuan pinjaman.

2. **Plot Regresi untuk Fitur yang Berkorelasi Kuat dengan Persetujuan Pinjaman**
   - Mengidentifikasi fitur-fitur penting seperti `MonthlyIncome`, `AnnualIncome`, `RiskScore`, `TotalDebtToIncomeRatio`, dan `InterestRate`.
   - Membuat plot regresi linear untuk masing-masing fitur terhadap `LoanApproved` untuk menggambarkan hubungan antara fitur-fitur ini dan kemungkinan persetujuan pinjaman.

Proses ini penting untuk mendapatkan wawasan awal dan mengarahkan langkah selanjutnya dalam pemodelan dan analisis lebih lanjut.

**[Rumusan Masalah]**

> 1. ***Bagaimana mengidentifikasi faktor-faktor utama yang mempengaruhi persetujuan pinjaman (LoanApproved) berdasarkan atribut-atribut personal dan finansial pemohon?***

## **5.1 Correlation between Features and Loan Approval**
"""

# Hapus kolom yang tidak relevan seperti ApplicationDate
df_numeric = cleaned_df.drop(columns=['ApplicationDate'])

# Konversi kolom kategorikal ke numerik menggunakan LabelEncoder
le = LabelEncoder()

for col in categorical_columns[1:]: # Tanpa memasukkan ApplicationDate
    df_numeric[col] = le.fit_transform(df_numeric[col])

correlation_matrix = df_numeric.corr() # Hitung korelasi hanya pada kolom numerik

correlation_matrix = correlation_matrix["LoanApproved"].sort_values(ascending=False) # Cetak korelasi dengan kolom LoanApproved

# Plot bar chart horizontal
plt.figure(figsize=(10, 6))
correlation_matrix.drop('LoanApproved').plot(kind='barh', color='skyblue')  # Drop kolom LoanApproved sendiri dari plot

# Menambahkan label dan judul
plt.xlabel('Correlation with LoanApproved')
plt.ylabel('Features')
plt.title('Correlation between Features and Loan Approval')

# Tampilkan plot
plt.tight_layout()
plt.show()

"""## **5.2 Regression plot for features that are strongly correlated with Loan Approved**"""

# List fitur penting
features = ['MonthlyIncome', 'AnnualIncome', 'RiskScore', 'TotalDebtToIncomeRatio', 'InterestRate']

# Buat plot regresi untuk setiap fitur
plt.figure(figsize=(15, 25))

for i, feature in enumerate(features, 1):
    plt.subplot(len(features), 1, i)  # Atur subplot
    sns.regplot(x=feature, y='LoanApproved', data=df_numeric, scatter_kws={"color": "blue"}, line_kws={"color": "red"})
    plt.title(f'Linear Regression: {feature} vs Loan Approved')

plt.tight_layout()
plt.show()

"""# **6. Data Preparation for Modeling**

Data preparation merupakan tahapan penting dalam proses pengembangan model machine learning. Ini adalah tahap di mana kita melakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan. Ada beberapa tahapan yang umum dilakukan pada data preparation, antara lain, seleksi fitur, transformasi data, feature engineering, dan dimensionality reduction.

Berikut adalah Tahap **Data Preparation** untuk pemodelan machine learning dari setiap sub-bagian:

1. **Encoding Fitur Kategori**
   - Melakukan **One-Hot Encoding** untuk fitur kategorikal menggunakan `pd.get_dummies()` untuk mengubah kategori menjadi variabel biner.

2. **Feature Scaling Menggunakan Min-Max Scaler**
   - Menggunakan **Min-Max Scaling** untuk menormalkan fitur ke rentang [0, 1], mencegah dominasi fitur dengan skala lebih besar, dan memperbaiki performa model berbasis jarak serta Neural Networks.

3. **Splitting Data**
   - Menghapus kolom `ApplicationDate`, memisahkan fitur (`X`) dan target (`y`), serta melakukan encoding pada label target.
   - Membagi dataset menjadi data latih dan data uji (80:20) dengan menjaga distribusi kelas yang seimbang.

**[Rumusan Masalah]**

> 2. ***Bagaimana algoritma prediktif dapat diimplementasikan untuk meningkatkan akurasi prediksi persetujuan pinjaman dan risiko kredit dengan memanfaatkan data historis dari pemohon?***

## **6.1 Encoding Fitur Kategori**
"""

# Lakukan One-Hot Encoding untuk fitur kategorikal
df_encoded = pd.get_dummies(cleaned_df, columns=categorical_columns[1:], drop_first=True)

print("Shape setelah One-Hot Encoding:", df_encoded.shape)

df_encoded.head()

"""## **6.2 Feature Scaling Using Min-Max Scaler**

**Min-Max Scaling** adalah teknik normalisasi yang mengubah fitur ke dalam skala tertentu, biasanya **antara 0 dan 1**. Tujuannya adalah untuk memastikan bahwa semua fitur berada dalam rentang yang sama, sehingga tidak ada fitur yang mendominasi fitur lainnya berdasarkan skala. Ini sangat penting dalam model yang sensitif terhadap skala, seperti **K-Nearest Neighbors (KNN)**, **Support Vector Machines (SVM)**, dan **Neural Networks**.

**<h3> Rumus Min-Max Scaling: </h3>**

$
X_{\text{scaled}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
$

- $X$ adalah nilai asli.
- $X_{\text{min}}$ dan $X_{\text{max}}$ adalah nilai minimum dan maksimum dari fitur tersebut.
- Hasilnya adalah nilai yang diubah ke dalam rentang [0, 1].

**<h3> Mengapa Menggunakan Min-Max Scaling? </h3>**

- **Menghindari dominasi fitur**: Jika suatu fitur memiliki skala yang jauh lebih besar, itu dapat mendominasi perhitungan jarak atau gradien, sehingga menurunkan performa model. dan cocok untuk data yang **memiliki distribusi tidak normal** atau tidak mengikuti distribusi Gaussian.
- **Model berbasis jarak**: Model seperti **KNN** atau **SVM** memerlukan fitur dengan skala yang sama agar perhitungan jarak antar data akurat.
- **Neural Networks**: Algoritma ini bekerja lebih baik dengan input yang terdistribusi dalam rentang yang sama, karena mempercepat proses pembelajaran.

Hasilnya adalah semua kolom numerik memiliki nilai antara **0 dan 1**, sehingga siap untuk digunakan dalam model machine learning yang peka terhadap skala fitur.
"""

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Dapatkan daftar kolom
features_to_scale = [col for col in df_encoded.columns if col not in ["LoanApproved", "ApplicationDate"]]

# Menampilkan daftar kolom yang telah diubah
print(features_to_scale)

# Fit and transform the data
df_scaled = df_encoded.copy()
df_scaled[features_to_scale] = scaler.fit_transform(df_scaled[features_to_scale])

df_scaled.sample(5)

"""## **6.3 Splitting Data**"""

df_scaled = df_scaled.drop(columns=["ApplicationDate"])

X = df_scaled.drop(columns=['LoanApproved'])
y = df_scaled['LoanApproved']

# Encode label strings into numbers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Membagi dataset menjadi training dan testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Membagi dataset menjadi training dan testing sets
X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
print("y_train_encoded shape:", y_train_encoded.shape)
print("y_test_encoded shape:", y_test_encoded.shape)

Counter(y_train_encoded), Counter(y_test_encoded)

"""# **7. Membangun Model Classification**

Berikut adalah penjelasan dari tahap **membangun model klasifikasi**:

1. **Kelas untuk Implementasi Train dan Evaluasi Model**
   - **`ModelEvaluation`**: Kelas ini memiliki metode untuk melatih dan mengevaluasi model, termasuk menghitung metrik seperti akurasi, presisi, recall, dan F1 Score.

2. **Inisialisasi Model**
   - Daftar model yang digunakan untuk klasifikasi meliputi:
     - AdaBoostClassifier
     - RidgeClassifier
     - LinearDiscriminantAnalysis
     - SVM dengan kernel linear
     - LogisticRegression
     - QuadraticDiscriminantAnalysis

3. **Pelatihan dan Evaluasi Model**
   - Setiap model dilatih dan dievaluasi, hasilnya disimpan dalam daftar.
   - Hasil yang ditampilkan mencakup metrik evaluasi dan waktu pelatihan.

4. **Visualisasi Hasil Evaluasi**
   - **Bar Chart**: Menampilkan perbandingan metrik evaluasi (Akurasi, Presisi, Recall, F1 Score) untuk semua model.
   - **Confusion Matrix**: Heatmap untuk setiap model menunjukkan kinerja prediksi.
   - **Bar Chart untuk Waktu Pelatihan**: Membandingkan waktu yang diperlukan untuk melatih masing-masing model.

Dengan demikian, tahap ini memungkinkan pemilihan model terbaik berdasarkan metrik yang relevan.

## **7.1 Build Class untuk Implementasi Train and Eval Model**
"""

class ModelEvaluation:
    def __init__(self, model, model_name):
        self.model = model
        self.model_name = model_name
        self.results = {}

    # Method untuk melatih model
    def train_model(self, X_train, y_train):
        # Record training time
        start_time = time.time()
        self.model.fit(X_train, y_train)
        end_time = time.time()
        train_time = end_time - start_time

        # Store training time in results
        self.results['Model'] = self.model_name
        self.results['Training Time (Sec)'] = train_time

    # Method untuk evaluasi model
    def evaluate_model(self, X_test, y_test):
        # Make predictions
        y_pred = self.model.predict(X_test)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        report = classification_report(y_test, y_pred)

        # Store results
        self.results["Accuracy"] = accuracy
        self.results["Precision"] = precision
        self.results["Recall"] = recall
        self.results["F1 Score"] = f1
        self.results["Confusion Matrix"] = cm
        self.results["Classification Report"] = report

        return self.results

"""## **7.2 Model Object Initiation**"""

# List dari model dan string untuk namanya
models = [
    (AdaBoostClassifier(), "Ada Boost Classifier"),
    (RidgeClassifier(), "Ridge Classifier"),
    (LinearDiscriminantAnalysis(), "Linear Discriminant Analysis"),
    (SVC(kernel='linear'), "SVM - Linear Kernel"),
    (LogisticRegression(), "Logistic Regression"),
    (QuadraticDiscriminantAnalysis(), "Quadratic Discriminant Analysis")
]

"""## **7.3 Train and Evaluation Model**"""

# List untuk menyimpan hasil evaluasi
evaluation_results = []

# Loop through each model and evaluate it
for model, model_name in models:
    evaluator = ModelEvaluation(model, model_name)
    evaluator.train_model(X_train, y_train)
    result = evaluator.evaluate_model(X_test, y_test)
    evaluation_results.append(result)

# Display results for each model
for result in evaluation_results:
    print(f"------- Model: {result['Model']} -------")
    print(f"Accuracy: {result['Accuracy']}")
    print(f"Precision: {result['Precision']}")
    print(f"Recall: {result['Recall']}")
    print(f"F1 Score: {result['F1 Score']}")
    print(f"Confusion Matrix:\n{result['Confusion Matrix']}")
    # print(f"Classification Report:\n{result['Classification Report']}")
    print(f"Training Time (Sec): {result['Training Time (Sec)']}\n")
    print("="*70, "\n")

"""## **7.4 Model Selection Based On Evaluation Metric**"""

# Convert evaluation results to a DataFrame for easier plotting
df_results = pd.DataFrame(evaluation_results)

# Reshape the DataFrame to a long format for Plotly Express
df_melted = df_results.melt(id_vars='Model', value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score'],
                            var_name='Metric', value_name='Score')

# Create the grouped bar chart using Plotly Express
fig = px.bar(df_melted,
             x='Model',
             y='Score',
             color='Metric',
             barmode='group',
             title='Model Evaluation Metrics Comparison')

# Customize the layout to place legend below the plot
fig.update_layout(
    legend=dict(
        orientation="h",  # Horizontal legend
        yanchor="bottom",
        y=-0.3,  # Position the legend below the plot
        xanchor="center",
        x=0.5
    ),
    xaxis_title='Models',
    yaxis_title='Scores',
    title_x=0.5  # Center the title
)

# Show the plot
fig.show()

num_models = len(evaluation_results) # Number of models

# Calculate the number of rows needed (2 columns per row)
num_cols = 2
num_rows = math.ceil(num_models / num_cols)

# Create subplots with 2 columns
fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 4))

# Flatten axes for easier indexing
axes = axes.flatten()

# Loop through each model and plot confusion matrix
for idx, result in enumerate(evaluation_results):
    model_name = result['Model']
    conf_matrix = result['Confusion Matrix']

    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="RdPu", cbar=False, ax=axes[idx])
    axes[idx].set_title(f'Confusion Matrix for {model_name}')
    axes[idx].set_xlabel('Predicted Label')
    axes[idx].set_ylabel('True Label')

# Remove empty subplots if any
for i in range(idx + 1, len(axes)):
    fig.delaxes(axes[i])

# Adjust layout
plt.tight_layout()
plt.show()

# Create a bar chart for Training Time
fig = px.bar(df_results,
             x='Model',
             y='Training Time (Sec)',
             title='Model Training Time Comparison',
             labels={'Training Time (Sec)': 'Training Time (Seconds)', 'Model': 'Models'})

# Customize the layout
fig.update_layout(
    xaxis_title='Models',
    yaxis_title='Training Time (Seconds)',
    title_x=0.5  # Center the title
)

# Show the plot
fig.show()

"""# **8. Kesimpulan**

## ***1. Bagaimana mengidentifikasi faktor-faktor utama yang mempengaruhi persetujuan pinjaman (LoanApproved) berdasarkan atribut-atribut personal dan finansial pemohon?***

Berdasarkan grafik di 5.1 dan 5.2, hasil interpretasi untuk mengidentifikasi faktor-faktor utama yang mempengaruhi persetujuan pinjaman (LoanApproved):

1. **Faktor yang Berhubungan Positif dengan Persetujuan Pinjaman**:
   - Berdasarkan plot regresi linear dan grafik korelasi, fitur **MonthlyIncome**, **AnnualIncome**, dan **CreditScore** memiliki korelasi positif dengan persetujuan pinjaman. Hal ini menunjukkan bahwa pemohon dengan pendapatan bulanan dan tahunan yang lebih tinggi serta skor kredit yang lebih baik memiliki peluang lebih besar untuk mendapatkan persetujuan pinjaman.
   - Faktor lain seperti **Experience** dan **Age** juga menunjukkan korelasi positif meskipun relatif kecil.

2. **Faktor yang Berhubungan Negatif dengan Persetujuan Pinjaman**:
   - **RiskScore** dan **TotalDebtToIncomeRatio** menunjukkan korelasi negatif yang signifikan dengan persetujuan pinjaman. Ini berarti semakin tinggi nilai **RiskScore** atau **rasio total hutang terhadap pendapatan**, semakin rendah kemungkinan pinjaman disetujui. Pemohon dengan risiko tinggi atau proporsi hutang yang besar terhadap pendapatan umumnya memiliki peluang lebih kecil.
   - **InterestRate** juga menunjukkan korelasi negatif, menandakan bahwa suku bunga yang lebih tinggi dapat mengurangi kemungkinan persetujuan pinjaman.

3. **Faktor dengan Pengaruh Signifikan**:
   - Berdasarkan grafik korelasi, faktor **RiskScore** memiliki korelasi negatif paling kuat, sementara **TotalDebtToIncomeRatio** dan **InterestRate** juga memiliki pengaruh signifikan dalam menentukan persetujuan pinjaman.
   - Sebaliknya, pendapatan bulanan dan tahunan memiliki korelasi positif yang kuat dengan persetujuan pinjaman, menunjukkan bahwa faktor kemampuan finansial menjadi pertimbangan utama.

Kesimpulannya, faktor-faktor utama yang mempengaruhi persetujuan pinjaman mencakup skor risiko (RiskScore), rasio hutang terhadap pendapatan (TotalDebtToIncomeRatio), suku bunga (InterestRate), serta pendapatan pemohon (MonthlyIncome dan AnnualIncome). Faktor-faktor ini dapat digunakan sebagai panduan dalam penentuan kebijakan persetujuan pinjaman.

## ***2. Bagaimana algoritma prediktif dapat diimplementasikan untuk meningkatkan akurasi prediksi persetujuan pinjaman dan risiko kredit dengan memanfaatkan data historis dari pemohon?***

Algoritma prediktif dapat diimplementasikan untuk meningkatkan akurasi prediksi persetujuan pinjaman dan risiko kredit dengan memanfaatkan data historis dari pemohon melalui langkah-langkah berikut:

1. **Pengumpulan dan Persiapan Data:** Data historis pemohon, seperti demografi, pendapatan, dan riwayat kredit, dikumpulkan. Data kemudian dibersihkan, fitur kategorikal diencode dengan metode OHE, dan fitur numerik dinormalisasi dengan Min-Max Scaler agar model dapat bekerja optimal.

2. **Pemilihan dan Pelatihan Model:** Model seperti **AdaBoostClassifier**, **RidgeClassifier**, dan **SVM** dilatih menggunakan data historis dan dilakukan Trial and Error. Lalu model dievaluasi untuk menemukan yang memberikan performa terbaik dalam prediksi persetujuan pinjaman.

3. **Evaluasi dan Validasi Model:** Model dievaluasi dengan data uji untuk mengukur kinerja menggunakan **confusion matrix**. Model dengan precision dan recall tinggi dipilih untuk mengurangi risiko kesalahan prediksi.

4. **Penggunaan Metrik Evaluasi:** Metrik seperti **accuracy**, **precision**, **recall**, **F1 score**, dan **Waktu Training** digunakan untuk memilih model terbaik. Ini membantu dalam mempertimbangkan risiko pada kedua kelas (disetujui atau ditolak).

5. **Manfaat dari Implementasi:**
   - Dengan memanfaatkan data historis secara efektif, model dapat **mengidentifikasi pola** dan faktor-faktor yang berkontribusi terhadap keberhasilan atau kegagalan pembayaran pinjaman. Ini memungkinkan lembaga keuangan untuk mengambil keputusan yang lebih baik, **mengurangi risiko kredit**, dan **meningkatkan efisiensi** proses persetujuan pinjaman.
   - Prediksi yang lebih akurat mengurangi kemungkinan memberikan pinjaman kepada pemohon dengan risiko gagal bayar yang tinggi, sehingga menurunkan **tingkat gagal bayar** dan meningkatkan **kualitas portofolio kredit**.

**Interpretasi Hasil Evaluasi Model:**

1. **AdaBoostClassifier, RidgeClassifier, Linear Discriminant Analysis, SVM - Linear Kernel:**
   - Semua model ini memiliki **accuracy, precision, recall, dan F1 score** sempurna (1.0), tanpa kesalahan klasifikasi. **RidgeClassifier** dan **Linear Discriminant Analysis** memiliki waktu pelatihan tercepat (0.061 dan 0.154 detik).
   
2. **Logistic Regression:**
   - **Accuracy** sangat tinggi (0.999), **precision** sempurna (1.0), tapi **recall** sedikit lebih rendah (0.9976). Ada 1 kesalahan klasifikasi dengan waktu pelatihan yang cukup cepat (0.188 detik).

3. **Quadratic Discriminant Analysis (QDA):**
   - **Accuracy** dan **F1 score** sangat tinggi (0.9989 dan 0.9976), dengan **recall** sempurna (1.0). Ada 2 kesalahan klasifikasi dan waktu pelatihan singkat (0.078 detik).

**Kesimpulan Model:**

- **Model-Model Terbaik:** Dari hasil evaluasi, **AdaBoostClassifier**, **RidgeClassifier**, **Linear Discriminant Analysis**, dan **SVM (Linear Kernel)** memiliki performa sempurna dalam prediksi persetujuan pinjaman. Keempat model ini memberikan akurasi yang sangat tinggi tanpa kesalahan klasifikasi.
- **Trade-off antara Akurasi dan Waktu Pelatihan:** Jika waktu pelatihan adalah pertimbangan utama, **RidgeClassifier** dan **Linear Discriminant Analysis** dapat dipilih karena memberikan akurasi sempurna dengan waktu pelatihan yang jauh lebih singkat dibandingkan **AdaBoostClassifier**. Namun, jika waktu pelatihan yang sedikit lebih lama dapat diterima, **AdaBoostClassifier** memberikan akurasi sempurna dan cenderung lebih robust dalam menangani data yang kompleks.

Implementasi algoritma ini dapat meningkatkan akurasi prediksi persetujuan pinjaman dengan memilih model yang memberikan performa terbaik sesuai kebutuhanâ€”baik dari segi akurasi maupun efisiensi waktu.
"""